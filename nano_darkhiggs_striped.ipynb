{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from striped.job import SinglePointStripedSession as Session, IPythonDisplay\n",
    "from vega import VegaLite\n",
    "\n",
    "from histbook import Hist, beside, groupby, below\n",
    "from histbook import bin as hbin\n",
    "\n",
    "\n",
    "import zlib\n",
    "import cloudpickle\n",
    "import fnal_column_analysis_tools\n",
    "import uproot\n",
    "import awkward\n",
    "import numpy as np\n",
    "#from xsec import xsecEval\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from dummy_distributions import dummy_jagged_eta_pt\n",
    "#import cloudpickle\n",
    "\n",
    "#import zlib\n",
    "\n",
    "#import weights as weights\n",
    "#import pickle\n",
    "#import zlib\n",
    "#import numpy as np\n",
    "\n",
    "\n",
    "#job_server = (\"ifdb01.fnal.gov\", 8765)\n",
    "\n",
    "#job_server = (\"dbwebdev.fnal.gov\", 8765)\n",
    "datasets = [\n",
    "           \"MET_NanoTuples-Data2017_Run2017C-31Mar2018-v1_2017\",\n",
    "           \"MET_NanoTuples-Data2017_Run2017B-31Mar2018-v1_2017\",\n",
    "           \"ST_s-channel_4f_InclusiveDecays_13TeV-amcatnlo-pythia8\",\n",
    "           \"WminusH_HToBB_WToLNu_M125_13TeV_powheg_pythia8\",\n",
    "           ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load histograms with lookup_tools\n",
    "#extractor = fnal_column_analysis_tools.lookup_tools.extractor()\n",
    "#extractor.add_weight_sets([\"testJson * EIDISO_WH_out.histo.json\"])\n",
    "#extractor.finalize()\n",
    "\n",
    "#extractor = fnal_column_analysis_tools.lookup_tools.extractor()\n",
    "#correctionDescriptions = open(\"newCorrectionFiles.txt\").readlines()\n",
    "#extractor.add_weight_sets(['* * moriond17/muon_scalefactors_37ifb.root'])\n",
    "#extractor.finalize()\n",
    "\n",
    "#weights_names = zlib.compress(cloudpickle.dumps(extractor._extractor__names))\n",
    "#weights_vals = zlib.compress(cloudpickle.dumps(extractor._extractor__weights))\n",
    "#evaluator = extractor.make_evaluator()\n",
    "#let's pickle and zip it\n",
    "#eval_pickle = cloudpickle.dumps(evaluator)\n",
    "#eval_pickle= zlib.compress(eval_pickle)\n",
    "\n",
    "\n",
    "#counts, test_eta, test_pt = dummy_jagged_eta_pt()\n",
    " \n",
    "#sf_out = evaluator['testJsonEIDISO_WH/eta_pt_ratio_value'](test_eta, test_pt)\n",
    "#sf_err_out = evaluator['testJsonEIDISO_WH/eta_pt_ratio_error'](test_eta, test_pt)\n",
    "#print(sf_out)\n",
    "#print(sf_err_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__worker_class__\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#sys = next(getattr(c, f).__func__.__globals__['sys'] for c in ().__class__.__base__.__subclasses__() for f in dir(c) if isinstance(getattr(c, f, None), type((lambda: 0).__get__(0))) and 'sys' in getattr(c, f).__func__.__globals__)\n",
    "#if 'sandbox' in sys.modules:\n",
    "#    __builtins__['__import__'] = sys.modules['sandbox'].saved_import\n",
    "\n",
    "\n",
    "def dummySF(eta,pt):\n",
    "    return 0.8\n",
    "import uproot, uproot_methods, awkward, numpy as np, zlib, cloudpickle\n",
    "import fnal_column_analysis_tools\n",
    "from fnal_column_analysis_tools.lookup_tools import evaluator \n",
    "from fnal_column_analysis_tools.analysis_objects import JaggedCandidateArray\n",
    "class Worker(object):\n",
    "\n",
    "    \n",
    "   # singleele_trigger_paths = [#\"HLT_Ele27_WPLoose_Gsf\",\n",
    "    #      \"HLT_Ele105_CaloIdVT_GsfTrkIdT\"]\n",
    "        #  \"HLT_Ele27_WPTight_Gsf\",\n",
    "          #\"HLT_Ele27_eta2p1_WPTight_Gsf\",\n",
    "         # \"HLT_Ele32_eta2p1_WPTight_Gsf\",]\n",
    "          #\"HLT_Ele35_WPLoose_Gsf\",\n",
    "          #\"HLT_ECALHT800\"]\n",
    "    \n",
    "    #singleele_trigger_columns = {path:path for path in singleele_trigger_paths}\n",
    "    electron_columns = {'pt':'Electron.pt','eta':'Electron.eta','phi':'Electron.phi','mass':'Electron.mass', \n",
    "                        'iso':'Electron.pfRelIso03_all','dxy':'Electron.dxy','dz':'Electron.dz',\n",
    "                        #'cutBased_HLTPreSel':'Electron.cutBased_HLTPreSel', # 'mvaSpring16GP_WP80':'Electron.mvaSpring16GP_WP80',\n",
    "                        'charge':'Electron.charge', # 'id':'Electron.mvaSpring16GP_WP90',\n",
    "                        'pdgId':'Electron.pdgId','deltaEtaSC':'Electron.deltaEtaSC'}\n",
    "    muon_columns = {'pt':'Muon.pt','eta':'Muon.eta','phi':'Muon.phi','mass':'Muon.mass','iso':'Muon.pfRelIso04_all',\n",
    "                    'dxy':'Muon.dxy','dz':'Muon.dz','charge':'Muon.charge','tightId':'Muon.tightId',\n",
    "                    'pdgId':'Muon.pdgId'}\n",
    "    jet_columns = {'pt':'Jet.pt','eta':'Jet.eta','phi':'Jet.phi','mass':'Jet.mass','id':'Jet.jetId'}\n",
    "    tau_columns = {'pt':'Tau.pt','eta':'Tau.eta','phi':'Tau.phi','mass':'Tau.mass','decayMode':'Tau.idDecayMode',\n",
    "                   'decayModeNew':'Tau.idDecayModeNewDMs'}#,'id':'Tau.idMVAnew'} # (idmVAnewDM does not exist in my file) idMVAnew\n",
    "    photon_columns = {'pt':'Photon.pt','eta':'Photon.eta','phi':'Photon.phi','mass':'Photon.mass'}\n",
    "   # gen_columns = {'pt':'GenPart.pt','eta':'GenPart.eta','phi':'GenPart.phi','mass':'GenPart.mass',\n",
    "                   #'id':'GenPart.pdgId','status':'GenPart.status', 'statusFlags':'GenPart.statusFlags',\n",
    "                   #'mIdx':'GenPart.genPartIdxMother'}\n",
    "    additional_columns = {\"nPhoton\":\"nPhoton\",\"nElectron\":\"nElectron\",\"nMuon\":\"nMuon\",\"nTau\":\"nTau\",\"nJet\":\"nJet\",\n",
    "                         }# \"nGenPart\":\"nGenPart\"}\n",
    "    all_columns = [tau_columns,electron_columns,muon_columns,jet_columns,photon_columns,additional_columns] #singleele_trigger_columns,\n",
    "\n",
    "    columns = []\n",
    "\n",
    "    \n",
    "    \n",
    "    for cols in all_columns: columns.extend(list(cols.values()))\n",
    "   \n",
    "    #Columns = [\"nJet\",\"nMuon\",\"nElectron\",\"Jet.pt\", \"MET_pt\",\"Electron.pt\",\"Electron.eta\",\"Electron.phi\",\n",
    "     #          \"Electron.dxy\", \"Electron.dz\",\"Electron.mass\", \"Electron.pfRelIso03_all\", \"Electron.mvaSpring16GP_WP90\",\n",
    "      #         \"Muon.pt\", \"Muon.eta\", \"Muon.phi\", \"Muon.mass\",\"Muon.dxy\", \"Muon.dz\", \"Muon.pfRelIso04_all\"   ]\n",
    "    Columns = columns\n",
    "    \n",
    "    \n",
    "    \n",
    "    def run(self, events, job, db):\n",
    "        electron_columns2 = {'pt':events.Electron.pt,'eta':events.Electron.eta,'phi':events.Electron.phi,\n",
    "                             'mass':events.Electron.mass,\n",
    "                             'iso':events.Electron.pfRelIso03_all,'dxy':events.Electron.dxy,'dz':events.Electron.dz,\n",
    "                            #'mvaSpring16GP_WP80':events.Electron.mvaSpring16GP_WP80, # 'cutBased_HLTPreSel':events.Electron.cutBased_HLTPreSel,\n",
    "                            'charge':events.Electron.charge,# 'id':events.Electron.mvaSpring16GP_WP90,\n",
    "                             'pdgId':events.Electron.pdgId,'deltaEtaSC':events.Electron.deltaEtaSC}\n",
    "        muon_columns2 = {'pt':events.Muon.pt,'eta':events.Muon.eta,'phi':events.Muon.phi,'mass':events.Muon.mass,\n",
    "                        'iso':events.Muon.pfRelIso04_all,\n",
    "                        'dxy':events.Muon.dxy,'dz':events.Muon.dz,'charge':events.Muon.charge,\n",
    "                        'tightId':events.Muon.tightId,'pdgId':events.Muon.pdgId}\n",
    "        jet_columns2 = {'pt':events.Jet.pt,'eta':events.Jet.eta,'phi':events.Jet.phi,'mass':events.Jet.mass,\n",
    "                        'id':events.Jet.jetId}\n",
    "        tau_columns2 = {'pt':events.Tau.pt,'eta':events.Tau.eta,'phi':events.Tau.phi,'mass':events.Tau.mass,\n",
    "                       'decayMode':events.Tau.idDecayMode,\n",
    "                       'decayModeNew':events.Tau.idDecayModeNewDMs}#,'id':events.Tau.idMVAnew} # (idmVAnewDM does not exist in my file) idMVAnew\n",
    "        photon_columns2 = {'pt':events.Photon.pt,'eta':events.Photon.eta,'phi':events.Photon.phi,'mass':events.Photon.mass}\n",
    "       # gen_columns2 = {'pt':events.GenPart.pt,'eta':events.GenPart.eta,'phi':events.GenPart.phi,'mass':events.GenPart.mass,\n",
    "                   #'id':events.GenPart.pdgId,'status':events.GenPart.status, 'statusFlags':events.GenPart.statusFlags,\n",
    "                   #'mIdx':events.GenPart.genPartIdxMother}\n",
    "        \n",
    "        weights  = np.ones(len(events))\n",
    "        \n",
    "       # job.message(str(eval(\"events.Electron.pt\"))+str(8))\n",
    "        #eval_pickle = job[\"evaluator\"]\n",
    "        #eval_pickle = zlib.decompress(eval_pickle)\n",
    "        #my_evaluator = cloudpickle.loads(eval_pickle)\n",
    "       # weights_names = cloudpickle.loads(zlib.decompress(job[\"weights_names\"]))\n",
    "       # weights_vals = cloudpickle.loads(zlib.decompress(job[\"weights_vals\"]))\n",
    "        #weights_eval = evaluator(cloudpickle.loads(zlib.decompress(job[\"weights_names\"])),\n",
    "         #                                 cloudpickle.loads(zlib.decompress(job[\"weights_vals\"])))\n",
    "        #weights_eval = evaluator(weights_names,weights_vals)\n",
    "        #job.message(str(dir(weights_eval)))\n",
    "       # triggers  = {'SingleEle':np.prod([arrays[val] for val in singleele_trigger_columns],axis=0)}\n",
    "      \n",
    "        electrons = JaggedCandidateArray.candidatesfromcounts(events.nElectron,\n",
    "                **{key:val for key,val in electron_columns2.items()})\n",
    "        muons     = JaggedCandidateArray.candidatesfromcounts(events.nMuon, \n",
    "                **{key:val for key,val in muon_columns2.items()})             \n",
    "        photons   = JaggedCandidateArray.candidatesfromcounts(events.nPhoton, \n",
    "               **{key:val for key,val in photon_columns2.items()})\n",
    "        jets      = JaggedCandidateArray.candidatesfromcounts(events.nJet, \n",
    "               **{key:val for key,val in jet_columns2.items()})\n",
    "        #genPart   = JaggedCandidateArray.candidatesfromcounts(events.nGenPart, \n",
    "         #      **{key:val for key,val in gen_columns2.items()})\n",
    "        taus      = JaggedCandidateArray.candidatesfromcounts(events.nTau, \n",
    "               **{key:val for key,val in tau_columns2.items()})\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "        #loose lepton selection\n",
    "            #loose electron selection\n",
    "        loose_e_selection = (electrons.pt>7)*(abs(electrons.eta)<2.4)*(abs(electrons.dxy)<0.05)*(abs(electrons.dz)<0.2)*(electrons.iso<0.4)#*(electrons.id)\n",
    "        loose_electrons = electrons[loose_e_selection]\n",
    "      \n",
    "            #loose muon selection\n",
    "        loose_m_selection = (muons.pt>5)*(abs(muons.eta)<2.4)*(abs(muons.dxy)<0.5)*(abs(muons.dz)<1)*(muons.iso<0.4)\n",
    "        loose_muons = muons[loose_m_selection]\n",
    "        \n",
    "        #                                          Lepton trigger sf\n",
    "                                                                        \n",
    "        e_counts = loose_electrons.counts\n",
    "        e_sfTrigg = np.ones(loose_electrons.size)\n",
    "        e_sfTrigg[e_counts>0] = 1 - dummySF(loose_electrons.eta[e_counts>0,0], loose_electrons.pt[e_counts > 0,0])\n",
    "        e_sfTrigg[e_counts > 1] =  1- (1- dummySF(loose_electrons.eta[e_counts>1,0], loose_electrons.pt[e_counts > 1,0]))*(1-dummySF(loose_electrons.eta[e_counts>1,1], loose_electrons.pt[e_counts > 1,1]))\n",
    "        dumJagged = loose_electrons.pt/loose_electrons.pt\n",
    "        weight = dumJagged*e_sfTrigg\n",
    "        \n",
    "        m_counts = loose_muons.counts\n",
    "        m_sfTrigg = np.ones(loose_muons.size)\n",
    "        m_sfTrigg[m_counts>0] = np.ones(m_sfTrigg.size)[m_counts>0] - dummySF(loose_muons.eta[m_counts>0,0], loose_muons.pt[m_counts > 0,0])\n",
    "        m_sfTrigg[m_counts > 1] =  1- (1- dummySF(loose_muons.eta[m_counts>1,0], loose_muons.pt[m_counts > 1,0]))*(1- dummySF(loose_muons.eta[m_counts>1,1], loose_muons.pt[m_counts > 1,1]))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #                                           Histogram variables\n",
    "        \n",
    "        weight = np.concatenate(weight)\n",
    "        \n",
    "        e_pt = loose_electrons.pt\n",
    "        e_pt_flat = np.concatenate(e_pt)\n",
    "        \n",
    "        m_pt = loose_muons.pt\n",
    "        m_pt_flat = np.concatenate(m_pt)\n",
    "        \n",
    "        e_energy = loose_electrons.p4.E\n",
    "        e_energy_flat = np.concatenate(e_energy) # Flattens jagged array\n",
    "        \n",
    "        m_energy = loose_muons.p4.E\n",
    "        m_energy_flat = np.concatenate(m_energy)\n",
    "        \n",
    "        e_eta = loose_electrons.eta\n",
    "        e_eta_flat = np.concatenate(e_eta)\n",
    "        \n",
    "        m_eta = loose_muons.eta\n",
    "        m_eta_flat = np.concatenate(m_eta)\n",
    "        \n",
    "       \n",
    "        #job.message(str(awkward.__version__))\n",
    "      \n",
    "        \n",
    "        \n",
    "        \n",
    "        #sf_out = evaluator['testJsonEIDISO_WH/eta_pt_ratio_value'](e_eta_flat, e_pt_flat)\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        dataset = dataset = job[\"dataset\"]\n",
    "        job.fill(dataset=dataset, Electron_pt=e_pt_flat, weight = weight)\n",
    "       # job.fill(dataset=dataset, Muon_pt=m_pt_flat)\n",
    "        #job.fill(dataset=dataset, Electron_E = e_energy_flat)\n",
    "        #job.fill(dataset=dataset, Muon_E=m_energy_flat)\n",
    "        #job.fill(dataset=dataset, Electron_eta=e_eta_flat)\n",
    "        #job.fill(dataset=dataset, Muon_eta=m_eta_flat)\n",
    "        #job.fill(electrons_ptdataset=dataset, weight=weight)\n",
    "        \n",
    "      \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ifdb02.fnal.gov', 8766)\n",
      "(u'131.225.193.17', 8776)\n"
     ]
    }
   ],
   "source": [
    "session = Session(\"/home/felipe/striped.yaml\")\n",
    "\n",
    "\n",
    "\n",
    "electron_pt = Hist(hbin(\"Electron_pt\", 70, 0, 800), groupby(\"dataset\"), weight=\"weight\")\n",
    "#muon_pt = Hist(hbin(\"Muon_pt\", 70, 0, 800), groupby(\"dataset\"))\n",
    "\n",
    "\n",
    "#electron_E = Hist(hbin(\"Electron_E\",70,0,1500),groupby(\"dataset\")) #, weight=\"weight\")\n",
    "#muon_E     = Hist(hbin(\"Muon_E\",70,0,1500),groupby(\"dataset\"))\n",
    "\n",
    "#electron_eta = Hist(hbin(\"Electron_eta\",50,-2.5,2.5),groupby(\"dataset\"))\n",
    "#muon_eta = Hist(hbin(\"Muon_eta\",50,-2.5,2.5),groupby(\"dataset\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "disp = below(\n",
    "    beside(\n",
    "        electron_pt\n",
    "            .stack(\"dataset\")\n",
    "            .area(\"Electron_pt\", width=300, yscale={\"type\":\"log\"})#,\n",
    "       # muon_pt\n",
    "       #     .stack(\"dataset\")\n",
    "       #     .area(\"Muon_pt\", width=300, yscale={\"type\":\"log\"})\n",
    "        #nelectrons\n",
    "         #   .stack(\"dataset\")\n",
    "          #  .area(\"nElectron\", width=200)\n",
    "    #),\n",
    "    #beside(\n",
    "    #    electron_E\n",
    "     #       .stack(\"dataset\")\n",
    "     #       .area(\"Electron_E\", width=300, yscale={\"type\":\"log\"}),\n",
    "     #   muon_E\n",
    "     #       .stack(\"dataset\")\n",
    "     #       .area(\"Muon_E\", width=300, yscale={\"type\":\"log\"})\n",
    "    #),\n",
    "    \n",
    "    #beside(\n",
    "    #    electron_eta\n",
    "    #        .stack(\"dataset\")\n",
    "    #        .area(\"Electron_eta\", width=300, yscale={\"type\":\"log\"}),\n",
    "    #    muon_eta\n",
    "    #        .stack(\"dataset\")\n",
    "    #        .area(\"Muon_eta\", width=300, yscale={\"type\":\"log\"})\n",
    "    \n",
    "    )\n",
    "    \n",
    ")\n",
    "    \n",
    "\n",
    "display = IPythonDisplay(\n",
    "    disp\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "class Callback:\n",
    "    \n",
    "    def __init__(self, display):\n",
    "        self.Display = display\n",
    "        \n",
    "    def on_histogram_update(self, nevents):\n",
    "        self.Display.update()\n",
    "    def on_stream_update(self,nEvents, data):\n",
    "        for x in data[\"a\"]:\n",
    "            print x\n",
    "    def on_message(self,workerId, nEvents, message):\n",
    "        print message\n",
    "    def on_exception(self,workerId, info):\n",
    "        print info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MET_NanoTuples-Data2017_Run2017C-31Mar2018-v1_2017                      52.998 M events,   0.625 M events/sec\n",
      "MET_NanoTuples-Data2017_Run2017B-31Mar2018-v1_2017                      47.375 M events,   0.627 M events/sec\n"
     ]
    }
   ],
   "source": [
    "display.init()\n",
    "callback = Callback(display)\n",
    "for dataset_name in datasets:\n",
    "    job = session.createJob(dataset_name, \n",
    "        user_callback=callback,\n",
    "        #user_params = {\"dataset\":dataset_name, \"weights_names\":weights_names,\"weights_vals\":weights_vals},\n",
    "        user_params = {\"dataset\":dataset_name },\n",
    "        histograms=[electron_pt]#,muon_pt,electron_E, muon_E, electron_eta, muon_eta]\n",
    "    )\n",
    "    job.run()\n",
    "    runtime = job.TFinish - job.TStart\n",
    "    nevents = job.EventsProcessed\n",
    "    print \"%-70s %7.3f M events, %7.3f M events/sec\" % (dataset_name, float(nevents)/1000000, nevents/runtime/1000000)\n",
    "\n",
    "disp.to(VegaLite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
